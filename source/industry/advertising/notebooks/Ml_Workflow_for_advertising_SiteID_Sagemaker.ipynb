{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adtech ML Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training data\n",
    "\n",
    "The first step is to load the training data into the dataframe. \n",
    "\n",
    "To do this we need to do the following:\n",
    "    1. Define the schema of the data\n",
    "    2. Load the data into a dataframe using spark's read functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Impressions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n",
    "# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "impressions = \"<%ImpressionsFile%>\"\n",
    "                      \n",
    "impdf = spark.read.parquet(impressions)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Activity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "activities = \"<%ActivitiesFile%>\"\n",
    "\n",
    "actdf = spark.read.parquet(activities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Records from Impression table for the given coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = impdf.select('User_ID','Browser_Platform_ID','Operating_System_ID','Ad_ID','Site_ID_DCM').dropDuplicates()\n",
    "imp_df.cache()\n",
    "imp_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Records from Activity table for the given coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_df = actdf.select('User_ID','Browser_Platform_ID','Operating_System_ID','Ad_ID','Site_ID_DCM').dropDuplicates()\n",
    "act_df.cache()\n",
    "act_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the records who are converted in Impression Table -> (Impressions All <-filter-> Activity = Converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_imp = imp_df.join(act_df,[\"User_ID\"]).filter(imp_df.Browser_Platform_ID == act_df.Browser_Platform_ID).filter(imp_df.Operating_System_ID == act_df.Operating_System_ID).filter(imp_df.Ad_ID == act_df.Ad_ID).filter(imp_df.Site_ID_DCM == act_df.Site_ID_DCM).drop(act_df.Browser_Platform_ID).drop(act_df.Operating_System_ID).drop(act_df.Ad_ID).drop(act_df.Site_ID_DCM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_imp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the records who are Not converted in Impression Table -> (Impressions All - Converted = Not Converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notconv_imp=imp_df.select('User_ID','Browser_Platform_ID','Operating_System_ID','Ad_ID','Site_ID_DCM').subtract(conv_imp.select('User_ID','Browser_Platform_ID','Operating_System_ID','Ad_ID','Site_ID_DCM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Converted Records - labeled converted with values 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_not_conv = notconv_imp.withColumn(\"purchased\",lit(0).cast(DoubleType()))\n",
    "imp_not_conv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converted Records - labeled converted with values 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_conv = conv_imp.withColumn(\"purchased\",lit(1).cast(DoubleType()))\n",
    "imp_conv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final  Impression table with the records (Converted Union NonConverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_final = imp_conv.unionAll(imp_not_conv)\n",
    "imp_final.printSchema()\n",
    "imp_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization - One Hot Encodeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "'''\n",
    "Utilize a StringIndexer to encode a column of strings to \n",
    "a column of indices.\n",
    "'''\n",
    "indexerA = StringIndexer()\n",
    "indexerA.setInputCol(\"Browser_Platform_ID\")\n",
    "indexerA.setOutputCol(\"browser_index\")\n",
    "#indexerA.setHandleInvalid(\"skip\")\n",
    "\n",
    "indexerB = StringIndexer()\n",
    "indexerB.setInputCol(\"Operating_System_ID\")\n",
    "indexerB.setOutputCol(\"os_index\")\n",
    "#indexerB.setHandleInvalid(\"skip\")\n",
    "\n",
    "indexerC = StringIndexer()\n",
    "indexerC.setInputCol(\"Ad_ID\")\n",
    "indexerC.setOutputCol(\"ad_index\")\n",
    "#indexerC.setHandleInvalid(\"skip\")\n",
    "\n",
    "indexerD = StringIndexer()\n",
    "indexerD.setInputCol(\"Site_ID_DCM\")\n",
    "indexerD.setOutputCol(\"site_id_dcm_index\")\n",
    "#indexerD.setHandleInvalid(\"skip\")\n",
    "\n",
    "'''\n",
    "Use the OneHotEncoder transformer to map a column of  \n",
    "indices to a column of binary vectors, where each vector\n",
    "has at most one dimension as non-zero (unity) value.\n",
    "'''\n",
    "encoderA = OneHotEncoder()\n",
    "encoderA.setInputCol(\"browser_index\")\n",
    "encoderA.setOutputCol(\"browser_features\")\n",
    "\n",
    "encoderB = OneHotEncoder()\n",
    "encoderB.setInputCol(\"os_index\")\n",
    "encoderB.setOutputCol(\"os_features\")\n",
    "\n",
    "encoderC = OneHotEncoder()\n",
    "encoderC.setInputCol(\"ad_index\")\n",
    "encoderC.setOutputCol(\"ad_features\")\n",
    "\n",
    "encoderD = OneHotEncoder()\n",
    "encoderD.setInputCol(\"site_id_dcm_index\")\n",
    "encoderD.setOutputCol(\"site_id_dcm_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexermodelA = indexerA.fit(imp_final)\n",
    "tempdf1 = indexermodelA.transform(imp_final)\n",
    "tempdf2 = encoderA.transform(tempdf1)\n",
    "\n",
    "indexermodelB = indexerB.fit(tempdf2)\n",
    "tempdf3 = indexermodelB.transform(tempdf2)\n",
    "tempdf4 = encoderB.transform(tempdf3)\n",
    "\n",
    "indexermodelC = indexerC.fit(tempdf4)\n",
    "tempdf5 = indexermodelC.transform(tempdf4)\n",
    "tempdf6 = encoderC.transform(tempdf5)\n",
    "\n",
    "indexermodelD = indexerD.fit(tempdf6)\n",
    "tempdf7 = indexermodelD.transform(tempdf6)\n",
    "encodedFinal = encoderD.transform(tempdf7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test , Train Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "samplingFraction = 0.40;\n",
    "trainingFraction = 0.60; testingFraction = (1-trainingFraction);\n",
    "seed = 1234;\n",
    "\n",
    "trainData, testData = imp_final.randomSplit([trainingFraction, testingFraction], seed=seed);\n",
    "\n",
    "# CACHE TRAIN AND TEST DATA\n",
    "trainData.cache()\n",
    "testData.cache()\n",
    "trainData.count(),testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_notconv = trainData.filter(\"purchased == 0.0\")\n",
    "filter_conv = trainData.filter(\"purchased == 1.0\")\n",
    "filter_notconv.count(),filter_conv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_notconv_sample = filter_notconv.sample(False, 0.001, 42)\n",
    "filter_notconv_sample.count()\n",
    "\n",
    "traindf = filter_notconv_sample.unionAll(filter_conv)\n",
    "traindf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the label distribution\n",
    "\n",
    "First we analyze the distribution of our target labels using a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "negcount = traindf.filter(\"purchased == 0.0\").count()\n",
    "poscount = traindf.filter(\"purchased == 1.0\").count()\n",
    "\n",
    "negfrac = 100*float(negcount)/float(negcount+poscount)\n",
    "posfrac = 100*float(poscount)/float(poscount+negcount)\n",
    "ind = [0.0,1.0]\n",
    "frac = [negfrac,posfrac]\n",
    "width = 0.35\n",
    "\n",
    "plt.title('Label Distribution')\n",
    "plt.bar(ind, frac, width, color='r')\n",
    "plt.xlabel(\"Purchased\")\n",
    "plt.ylabel('Percentage share')\n",
    "plt.xticks(ind,['0.0','1.0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "negcount = testData.filter(\"purchased == 0.0\").count()\n",
    "poscount = testData.filter(\"purchased == 1.0\").count()\n",
    "\n",
    "negfrac = 100*float(negcount)/float(negcount+poscount)\n",
    "posfrac = 100*float(poscount)/float(poscount+negcount)\n",
    "ind = [0.0,1.0]\n",
    "frac = [negfrac,posfrac]\n",
    "width = 0.35\n",
    "\n",
    "plt.title('Label Distribution')\n",
    "plt.bar(ind, frac, width, color='r')\n",
    "plt.xlabel(\"Purchased\")\n",
    "plt.ylabel('Percentage share')\n",
    "plt.xticks(ind,['0.0','1.0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We utilize a LogisticRegression classsifier for our ML model. \n",
    "\n",
    "VectorAssembler Transformer is used to collect the transformers and Estimators into a Single feature Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler()\n",
    "assembler.setInputCols([\"browser_features\", \"os_features\",\"ad_features\",\"site_id_dcm_features\"])\n",
    "assembler.setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker_pyspark import IAMRole, S3DataPath\n",
    "from sagemaker_pyspark.algorithms import *\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker_pyspark import *\n",
    "\n",
    "estimator = LinearLearnerBinaryClassifier(\n",
    "    sagemakerRole=IAMRole(role),\n",
    "    trainingInstanceType=\"ml.p2.8xlarge\",\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType=\"ml.m4.xlarge\",\n",
    "    endpointInitialInstanceCount=1,\n",
    "    trainingSparkDataFormatOptions={\n",
    "        \"featuresColumnName\":\"features\",\n",
    "        \"labelColumnName\":\"purchased\"\n",
    "    },\n",
    "    namePolicyFactory=RandomNamePolicyFactory(\"adClassF22\"),\n",
    "    endpointCreationPolicy=EndpointCreationPolicy.CREATE_ON_TRANSFORM)\n",
    "\n",
    "estimator.setFeatureDim(25)\n",
    "estimator.setLoss('logistic')\n",
    "estimator.binary_classifier_model_selection_criteria='recall_at_target_precision'\n",
    "estimator.target_recall=0.9\n",
    "estimator.positive_example_weight_mult='balanced'\n",
    "estimator.epochs=40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up an ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "trainingpipeline = Pipeline(stages=[indexerA,encoderA,\n",
    "                                    indexerB,encoderB,\n",
    "                                    indexerC,encoderC, \n",
    "                                    indexerD,encoderD,\n",
    "                                    assembler,estimator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = trainingpipeline.fit(trainData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataPredictions = model.transform(testData)\n",
    "testDataPredictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the Model performance\n",
    "\n",
    "Having trained the model, the next step is to evaluate its performance on the test data. This is achieved as follows:\n",
    "\n",
    "    Load the test data into a dataframe.\n",
    "    Score the test dataframe using the model.\n",
    "    Evaluate the Confustion Matrix\n",
    "    Compute the AUC metric based on the model scores on the test data.\n",
    "    Plot the ROC curve to better understand the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "outdataframe = testDataPredictions.select(\"purchased\",\"predicted_label\")\n",
    "pandadf = outdataframe.toPandas()\n",
    "npmat = pandadf.as_matrix()\n",
    "labels = npmat[:,0]\n",
    "predicted_label = npmat[:,1]\n",
    "\n",
    "cnf_matrix = confusion_matrix(labels, predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Purchased')\n",
    "    plt.xlabel('Predicted \\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_matrix,\n",
    "                      normalize    = False,\n",
    "                      target_names = ['Positive', 'Negative'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "# Instantiate metrics object\n",
    "predictionAndLabels = testDataPredictions.rdd.map(lambda lp: (float(lp.score), lp.purchased))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "'''\n",
    "We convert the output dataframe, with only the necessary columns to a\n",
    "pandas dataframe and convert that to a numpy matrix.\n",
    "'''\n",
    "outdf = predresscore.select(\"purchased\",\"score\")\n",
    "pdf = outdf.toPandas()\n",
    "npmat = pdf.as_matrix()\n",
    "labels = npmat[:,0]\n",
    "probs = npmat[:,1]\n",
    "\n",
    "'''\n",
    "We compute the false positive rate and true positive rate at various thresholds\n",
    "of the probability score and use that to recompute the auc and finally to \n",
    "plot the ROC curve.\n",
    "'''\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(labels, probs)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision = average_precision_score(labels, probs)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "         average_precision))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testData.unpersist()\n",
    "# trainData.unpersist()\n",
    "# imp_df.unpersist()\n",
    "# act_df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
